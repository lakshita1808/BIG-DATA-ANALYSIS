# BIG-DATA-ANALYSIS
"COMPANY": CODTECH IT SOLUTIONS
"NAME": LAKSHITA SEHGAL
"INTERN ID": CT04DG2051
"DOMAIN": DATA ANALYTICS
"DURATION": 4 WEEKS
"MENTOR": NEELA SANTOSH

In today’s data-driven world, analyzing large-scale datasets has become essential to uncover valuable insights and support data-informed decisions. This project, titled Big Data Analysis using PySpark, was carried out as part of Internship Task 1 for CODTECH. The primary goal of this project was to perform scalable data analysis using PySpark — a powerful big data processing framework — on a large dataset containing 30,000 records related to job roles, industries, salaries, and the impact of artificial intelligence (AI) and automation.

The dataset consisted of various fields such as Job Title, Industry, AI Impact Level, Median Salary, Experience Required, Gender Diversity, Remote Work Ratio, Job Openings (2024), Projected Openings (2030), and Automation Risk (%). These attributes were explored in depth to understand how the job market is evolving in the context of technology and automation.

The project began with data cleaning, where we inspected the dataset structure, number of rows and columns, data types, and any missing or incorrect values. Once the data was cleaned and ready, we moved on to exploratory data analysis (EDA) using PySpark's built-in functions like groupBy(), select(), count(), orderBy(), and conditional filtering. These operations helped us explore patterns in the data efficiently, even with its large size.

One of the key aspects analyzed was the AI Impact Level on job roles. We categorized jobs into three risk levels: Low, Moderate, and High. Interestingly, the dataset revealed that almost one-third of jobs fall under High AI Impact, indicating a significant portion of roles could potentially be automated or replaced in the coming years. At the same time, another one-third of jobs had Low impact, which are likely to remain future-safe due to human-centric skills they require.

We also examined Automation Risk vs Salary, discovering that some high-paying jobs (e.g., fast food manager, meteorologist) were at surprisingly high risk, while certain lower-paying roles were safer. Top growing job roles based on projected openings till 2030 were identified, including Academic Librarian, Surgeon, and Technical Sales Engineer. Additionally, gender diversity was analyzed across roles to assess inclusivity.

To support these insights visually, a bar graph was created using Seaborn and Matplotlib in Colab, showing job distribution across AI impact levels. Though visualization was optional, it added significant value in representing trends clearly and concisely.

In conclusion, this project effectively demonstrated how PySpark can be used for scalable and efficient big data analysis. It provided valuable insights into the future of work, showing which careers are growing, which are at risk, and how AI is reshaping the employment landscape. This task not only strengthened practical understanding of PySpark but also built a real-world perspective on technology's impact on jobs.

 Tools Used:
Google Colab (Python-based cloud platform)

PySpark (Big data analysis)

Pandas (for conversions)

Matplotlib & Seaborn (for visualization)
